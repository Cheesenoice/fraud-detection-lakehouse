# üìñ H∆Ø·ªöNG D·∫™N THI·∫æT L·∫¨P CHI TI·∫æT

## Lakehouse Project - Credit Card Fraud Detection

T√†i li·ªáu n√†y h∆∞·ªõng d·∫´n chi ti·∫øt **2 c√°ch ch·∫°y** d·ª± √°n Lakehouse:

1. **C√°ch 1**: Full Pipeline Script (T·ª± ƒë·ªông h√≥a ho√†n to√†n)
2. **C√°ch 2**: Interactive Jupyter Notebooks (H·ªçc t·∫≠p & Kh√°m ph√°)

---

## üìã M·ª•c L·ª•c

1. [Y√™u C·∫ßu H·ªá Th·ªëng](#1-y√™u-c·∫ßu-h·ªá-th·ªëng)
2. [Chu·∫©n B·ªã Data](#2-chu·∫©n-b·ªã-data)
3. [C√°ch 1: Full Pipeline Script](#3-c√°ch-1-full-pipeline-script)
4. [C√°ch 2: Jupyter Notebooks](#4-c√°ch-2-jupyter-notebooks)
5. [Iceberg Time Travel Demo](#5-iceberg-time-travel-demo)
6. [T·∫°o Dashboard Trong Superset](#6-t·∫°o-dashboard-trong-superset)
7. [D·ª´ng H·ªá Th·ªëng](#7-d·ª´ng-h·ªá-th·ªëng)
8. [X·ª≠ L√Ω S·ª± C·ªë](#8-x·ª≠-l√Ω-s·ª±-c·ªë)

---

## 1. Y√™u C·∫ßu H·ªá Th·ªëng

### 1.1 Ph·∫ßn c·ª©ng

| Th√†nh ph·∫ßn | T·ªëi thi·ªÉu  | Khuy·∫øn ngh·ªã |
| ---------- | ---------- | ----------- |
| **RAM**    | 8GB        | 16GB        |
| **Disk**   | 15GB tr·ªëng | 25GB tr·ªëng  |
| **CPU**    | 4 cores    | 8 cores     |

### 1.2 Ph·∫ßn m·ªÅm

- **Docker Desktop** (bao g·ªìm Docker Compose v2)
- **Python 3.8+** v·ªõi `requests` library
- **Git** (t√πy ch·ªçn - ƒë·ªÉ clone repo)

### 1.3 Ki·ªÉm tra c√†i ƒë·∫∑t

```bash
# Ki·ªÉm tra Docker
docker --version
# Output: Docker version 24.x.x ho·∫∑c cao h∆°n

docker compose version
# Output: Docker Compose version v2.x.x

# Ki·ªÉm tra Python
python3 --version
# Output: Python 3.8+ ho·∫∑c cao h∆°n

# C√†i requests n·∫øu ch∆∞a c√≥
pip3 install requests
```

---

## 2. Chu·∫©n B·ªã Data

### 2.1 V·ªã tr√≠ data files

ƒê·∫∑t c√°c file CSV trong th∆∞ m·ª•c `notebooks/data/`:

```
Lakehouse_Project/
‚îî‚îÄ‚îÄ notebooks/
    ‚îî‚îÄ‚îÄ data/
        ‚îú‚îÄ‚îÄ train_transaction.csv   ‚Üê 590,540 records (b·∫Øt bu·ªôc)
        ‚îú‚îÄ‚îÄ train_identity.csv      ‚Üê 144,233 records (b·∫Øt bu·ªôc)
        ‚îú‚îÄ‚îÄ test_transaction.csv    ‚Üê (t√πy ch·ªçn)
        ‚îî‚îÄ‚îÄ test_identity.csv       ‚Üê (t√πy ch·ªçn)
```

### 2.2 Ki·ªÉm tra data

```bash
cd /path/to/Lakehouse_Project

# Ki·ªÉm tra files t·ªìn t·∫°i
ls -la notebooks/data/*.csv

# Ki·ªÉm tra s·ªë d√≤ng
wc -l notebooks/data/train_transaction.csv
# Output: ~590,541 (bao g·ªìm header)

wc -l notebooks/data/train_identity.csv
# Output: ~144,234 (bao g·ªìm header)
```

### 2.3 Ngu·ªìn data

Data t·ª´ cu·ªôc thi **IEEE-CIS Fraud Detection** tr√™n Kaggle:

- https://www.kaggle.com/c/ieee-fraud-detection/data

---

## 3. C√°ch 1: Full Pipeline Script

### ‚≠ê ƒê√¢y l√† c√°ch nhanh nh·∫•t - ch·ªâ c·∫ßn 1 l·ªánh!

### 3.1 Ch·∫°y Pipeline

```bash
cd /path/to/Lakehouse_Project

# Ch·∫°y to√†n b·ªô pipeline
./scripts/run_full_pipeline.sh
```

### 3.2 C√°c b∆∞·ªõc t·ª± ƒë·ªông th·ª±c hi·ªán

Script s·∫Ω t·ª± ƒë·ªông th·ª±c hi·ªán **5 b∆∞·ªõc** sau:

#### Step 0: Kh·ªüi ƒë·ªông Docker Stack (~2 ph√∫t)

- Kh·ªüi ƒë·ªông 7 Docker containers
- ƒê·ª£i t·∫•t c·∫£ services s·∫µn s√†ng
- Kh·ªüi ƒë·ªông Spark Thrift Server (port 10000)

```
Containers kh·ªüi ƒë·ªông:
‚îú‚îÄ‚îÄ minio          ‚Üí S3-compatible storage (port 9000, 9001)
‚îú‚îÄ‚îÄ minio-init     ‚Üí T·∫°o bucket warehouse
‚îú‚îÄ‚îÄ iceberg-rest   ‚Üí Iceberg REST Catalog (port 8181)
‚îú‚îÄ‚îÄ spark-iceberg  ‚Üí Spark + Jupyter (port 8888, 10000)
‚îú‚îÄ‚îÄ clickhouse     ‚Üí OLAP Database (port 8123)
‚îú‚îÄ‚îÄ superset       ‚Üí Visualization (port 8088)
‚îî‚îÄ‚îÄ dbt            ‚Üí Data transformation
```

#### Step 1: Bronze Layer (~2 ph√∫t)

- T·∫°o namespaces: `demo.bronze`, `bronze_silver`, `bronze_gold`
- ƒê·ªçc CSV files t·ª´ `notebooks/data/`
- Ingest v√†o Iceberg tables v·ªõi metadata columns

```
Output:
‚îú‚îÄ‚îÄ demo.bronze.transactions  ‚Üí 590,540 records
‚îî‚îÄ‚îÄ demo.bronze.identity      ‚Üí 144,233 records
```

#### Step 2: dbt run (~1 ph√∫t)

- Ch·∫°y Silver models (2 models)
- Ch·∫°y Gold models (8 models)
- T·ªïng c·ªông 10 models

```
Output:
‚îú‚îÄ‚îÄ Silver Layer:
‚îÇ   ‚îú‚îÄ‚îÄ bronze_silver.silver_transactions
‚îÇ   ‚îî‚îÄ‚îÄ bronze_silver.silver_identity
‚îÇ
‚îî‚îÄ‚îÄ Gold Layer:
    ‚îú‚îÄ‚îÄ bronze_gold.daily_transaction_summary
    ‚îú‚îÄ‚îÄ bronze_gold.fraud_by_card_type
    ‚îú‚îÄ‚îÄ bronze_gold.fraud_by_product
    ‚îú‚îÄ‚îÄ bronze_gold.hourly_fraud_analysis
    ‚îú‚îÄ‚îÄ bronze_gold.high_risk_transactions
    ‚îú‚îÄ‚îÄ bronze_gold.kpi_summary
    ‚îú‚îÄ‚îÄ bronze_gold.fraud_by_day_of_week
    ‚îî‚îÄ‚îÄ bronze_gold.fraud_by_amount_category
```

#### Step 3: Serving Layer (~1 ph√∫t)

- T·∫°o database `fraud_detection` trong ClickHouse
- Copy 8 Gold tables sang ClickHouse

```
Output:
‚îú‚îÄ‚îÄ fraud_detection.fraud_by_card_type        ‚Üí 15 rows
‚îú‚îÄ‚îÄ fraud_detection.hourly_fraud_analysis     ‚Üí 24 rows
‚îú‚îÄ‚îÄ fraud_detection.fraud_by_product          ‚Üí 5 rows
‚îú‚îÄ‚îÄ fraud_detection.kpi_summary               ‚Üí 1 row
‚îú‚îÄ‚îÄ fraud_detection.daily_transaction_summary ‚Üí 182 rows
‚îú‚îÄ‚îÄ fraud_detection.high_risk_transactions    ‚Üí 10,000 rows
‚îú‚îÄ‚îÄ fraud_detection.fraud_by_day_of_week      ‚Üí 7 rows
‚îî‚îÄ‚îÄ fraud_detection.fraud_by_amount_category  ‚Üí 6 rows
```

#### Step 4: Superset Auto-Setup (~2 ph√∫t)

- C√†i ƒë·∫∑t clickhouse-connect driver
- T·∫°o Database connection
- T·∫°o 8 Datasets
- T·∫°o 8 Charts
- T·∫°o 1 Dashboard v·ªõi layout

```
Output:
‚îú‚îÄ‚îÄ Database: ClickHouse Fraud Detection
‚îú‚îÄ‚îÄ Datasets: 8 datasets
‚îú‚îÄ‚îÄ Charts: 8 charts
‚îî‚îÄ‚îÄ Dashboard: Fraud Detection Dashboard
```

### 3.3 K·∫øt qu·∫£ mong ƒë·ª£i

```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    üéâ PIPELINE HO√ÄN T·∫§T!                             ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
üìä K·∫æT QU·∫¢:
   ‚úÖ Bronze Layer: Raw CSV ‚Üí Iceberg tables (demo.bronze.*)
   ‚úÖ Silver Layer: Cleaned data (bronze_silver.*)
   ‚úÖ Gold Layer: Analytics tables (bronze_gold.*)
   ‚úÖ Serving Layer: ClickHouse tables (fraud_detection.*)

üåê TRUY C·∫¨P:
   üìä Superset Dashboard: http://localhost:8088 (admin/admin)
   üìÅ MinIO Console:     http://localhost:9001 (admin/password123)
   üíª Jupyter:           http://localhost:8888
   üóÑÔ∏è  ClickHouse:        http://localhost:8123
```

### 3.4 Truy c·∫≠p Dashboard

1. M·ªü browser: **http://localhost:8088**
2. ƒêƒÉng nh·∫≠p: `admin` / `admin`
3. V√†o **Dashboards** ‚Üí **Fraud Detection Dashboard**

---

## 4. C√°ch 2: Jupyter Notebooks

### ‚≠ê C√°ch n√†y ph√π h·ª£p ƒë·ªÉ h·ªçc v√† kh√°m ph√° t·ª´ng b∆∞·ªõc

### 4.1 Kh·ªüi ƒë·ªông h·ªá th·ªëng

```bash
cd /path/to/Lakehouse_Project

# Kh·ªüi ƒë·ªông Docker + Thrift Server
./scripts/start_lakehouse.sh
```

**ƒê·ª£i ~2 ph√∫t** ƒë·ªÉ t·∫•t c·∫£ services kh·ªüi ƒë·ªông.

### 4.2 M·ªü Jupyter Lab

Truy c·∫≠p: **http://localhost:8888**

### 4.3 Notebook 1: Bronze Layer

**File**: `01_bronze_layer.ipynb`

**M·ª•c ƒë√≠ch**: Ingest d·ªØ li·ªáu th√¥ t·ª´ CSV v√†o Iceberg tables

**C√°c b∆∞·ªõc trong notebook**:

```python
# Cell 1: Import libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp, lit

# Cell 2: Kh·ªüi t·∫°o Spark Session v·ªõi Iceberg
spark = SparkSession.builder \
    .appName("Bronze Layer") \
    .config("spark.sql.catalog.demo", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.demo.type", "rest") \
    .config("spark.sql.catalog.demo.uri", "http://iceberg-rest:8181") \
    .config("spark.sql.catalog.demo.io-impl", "org.apache.iceberg.aws.s3.S3FileIO") \
    .config("spark.sql.catalog.demo.warehouse", "s3://warehouse/") \
    .config("spark.sql.catalog.demo.s3.endpoint", "http://minio:9000") \
    .getOrCreate()

# Cell 3: ƒê·ªçc CSV
transactions_df = spark.read.option("header", True).csv("/home/spark/notebooks/data/train_transaction.csv")
identity_df = spark.read.option("header", True).csv("/home/spark/notebooks/data/train_identity.csv")

# Cell 4: Th√™m metadata columns
transactions_df = transactions_df \
    .withColumn("_ingestion_time", current_timestamp()) \
    .withColumn("_source_file", lit("train_transaction.csv"))

# Cell 5: T·∫°o Iceberg tables
transactions_df.writeTo("demo.bronze.transactions").createOrReplace()
identity_df.writeTo("demo.bronze.identity").createOrReplace()

# Cell 6: Ki·ªÉm tra k·∫øt qu·∫£
spark.sql("SELECT COUNT(*) FROM demo.bronze.transactions").show()
spark.sql("SELECT COUNT(*) FROM demo.bronze.identity").show()
```

**Output**:

```
demo.bronze.transactions ‚Üí 590,540 records
demo.bronze.identity     ‚Üí 144,233 records
```

---

### 4.4 Notebook 2: Silver Layer

**File**: `02_silver_layer.ipynb`

**M·ª•c ƒë√≠ch**: L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu

**C√°c b∆∞·ªõc trong notebook**:

```python
# Cell 1: ƒê·ªçc Bronze tables
transactions_df = spark.table("demo.bronze.transactions")
identity_df = spark.table("demo.bronze.identity")

# Cell 2: Data cleaning
# - X·ª≠ l√Ω null values
# - Standardize data types
# - Remove duplicates

# Cell 3: Feature engineering
# - T·∫°o c√°c derived columns
# - Categorization

# Cell 4: Ghi v√†o Silver layer
cleaned_transactions.writeTo("demo.silver.silver_transactions").createOrReplace()
cleaned_identity.writeTo("demo.silver.silver_identity").createOrReplace()
```

**Output**:

```
demo.silver.silver_transactions ‚Üí Cleaned transaction data
demo.silver.silver_identity     ‚Üí Cleaned identity data
```

> üí° **Thay th·∫ø**: C√≥ th·ªÉ d√πng `dbt run` thay cho notebook n√†y

---

### 4.5 Notebook 3: Gold Layer

**File**: `03_gold_layer.ipynb`

**M·ª•c ƒë√≠ch**: T·∫°o c√°c b·∫£ng ph√¢n t√≠ch v√† aggregations

**C√°c b∆∞·ªõc trong notebook**:

```python
# Cell 1: ƒê·ªçc Silver tables
silver_trans = spark.table("demo.silver.silver_transactions")
silver_identity = spark.table("demo.silver.silver_identity")

# Cell 2: Join tables
enriched_df = silver_trans.join(silver_identity, "TransactionID", "left")

# Cell 3: T·∫°o aggregations
# - Daily summary
# - Fraud by card type
# - Fraud by product
# - Hourly analysis
# - KPI summary

# Cell 4: Ghi v√†o Gold layer
daily_summary.writeTo("demo.gold.daily_transaction_summary").createOrReplace()
fraud_by_card.writeTo("demo.gold.fraud_by_card_type").createOrReplace()
# ... c√°c tables kh√°c
```

**Output**:

```
demo.gold.daily_transaction_summary ‚Üí T·ªïng h·ª£p theo ng√†y
demo.gold.fraud_by_card_type        ‚Üí Ph√¢n t√≠ch theo lo·∫°i th·∫ª
demo.gold.fraud_by_product          ‚Üí Ph√¢n t√≠ch theo s·∫£n ph·∫©m
demo.gold.hourly_fraud_analysis     ‚Üí Ph√¢n t√≠ch theo gi·ªù
demo.gold.high_risk_transactions    ‚Üí Giao d·ªãch r·ªßi ro cao
demo.gold.kpi_summary               ‚Üí T·ªïng h·ª£p KPIs
```

> üí° **Thay th·∫ø**: C√≥ th·ªÉ d√πng `dbt run` thay cho notebook n√†y

---

### 4.6 Notebook 4: Serving Layer

**File**: `04_serving_layer.ipynb`

**M·ª•c ƒë√≠ch**: Copy d·ªØ li·ªáu Gold sang ClickHouse

**C√°c b∆∞·ªõc trong notebook**:

```python
# Cell 1: Import clickhouse-driver
import clickhouse_driver

# Cell 2: K·∫øt n·ªëi ClickHouse
client = clickhouse_driver.Client(
    host='clickhouse',
    port=9000,
    user='default',
    password='clickhouse123'
)

# Cell 3: T·∫°o database
client.execute("CREATE DATABASE IF NOT EXISTS fraud_detection")

# Cell 4: ƒê·ªçc Gold tables v√† copy sang ClickHouse
gold_tables = [
    "fraud_by_card_type",
    "hourly_fraud_analysis",
    "fraud_by_product",
    "kpi_summary",
    "daily_transaction_summary",
    "high_risk_transactions"
]

for table in gold_tables:
    # ƒê·ªçc t·ª´ Iceberg
    df = spark.table(f"demo.gold.{table}")

    # Copy sang ClickHouse
    # (implementation chi ti·∫øt trong notebook)
```

**Output**:

```
fraud_detection.fraud_by_card_type        ‚Üí 15 rows
fraud_detection.hourly_fraud_analysis     ‚Üí 24 rows
fraud_detection.fraud_by_product          ‚Üí 5 rows
fraud_detection.kpi_summary               ‚Üí 1 row
fraud_detection.daily_transaction_summary ‚Üí 182 rows
fraud_detection.high_risk_transactions    ‚Üí 10,000 rows
```

---

## 5. Iceberg Time Travel Demo

### üìì Notebook 5: Time Travel (`05_time_travel_demo.ipynb`)

**M·ª•c ƒë√≠ch**: Tr·ª±c quan h√≥a t√≠nh nƒÉng **Time Travel** ƒë·ªôc ƒë√°o c·ªßa Apache Iceberg

### 5.1 Xem l·ªãch s·ª≠ thay ƒë·ªïi

```python
# Xem history c·ªßa table
spark.sql("""
    SELECT * FROM demo.bronze.transactions.history
    ORDER BY made_current_at DESC
""").show()
```

**Output**:

```
+--------------------+-------------------+-------------------+
|made_current_at     |snapshot_id        |parent_id          |
+--------------------+-------------------+-------------------+
|2026-02-02 13:40:10 |8940796255292973358|null               |
+--------------------+-------------------+-------------------+
```

### 5.2 Xem danh s√°ch Snapshots

```python
# Xem t·∫•t c·∫£ snapshots
spark.sql("""
    SELECT
        snapshot_id,
        committed_at,
        operation,
        summary
    FROM demo.bronze.transactions.snapshots
""").show(truncate=False)
```

### 5.3 Query d·ªØ li·ªáu t·∫°i th·ªùi ƒëi·ªÉm c·ª• th·ªÉ

```python
# Query d·ªØ li·ªáu t·∫°i snapshot c·ª• th·ªÉ
snapshot_id = 8940796255292973358

df_old = spark.read \
    .option("snapshot-id", snapshot_id) \
    .table("demo.bronze.transactions")

df_old.count()
```

### 5.4 Query theo timestamp

```python
# Query d·ªØ li·ªáu t·∫°i th·ªùi ƒëi·ªÉm c·ª• th·ªÉ
df_at_time = spark.read \
    .option("as-of-timestamp", "2026-02-02 13:00:00") \
    .table("demo.bronze.transactions")
```

### 5.5 So s√°nh 2 versions

```python
# ƒê·ªçc 2 versions kh√°c nhau
old_snapshot = 8940796255292973350
new_snapshot = 8940796255292973358

df_old = spark.read.option("snapshot-id", old_snapshot).table("demo.bronze.transactions")
df_new = spark.read.option("snapshot-id", new_snapshot).table("demo.bronze.transactions")

# So s√°nh s·ªë l∆∞·ª£ng records
print(f"Old snapshot: {df_old.count()} records")
print(f"New snapshot: {df_new.count()} records")
print(f"Difference: {df_new.count() - df_old.count()} records")
```

### 5.6 Rollback v·ªÅ version tr∆∞·ªõc

```python
# Rollback table v·ªÅ snapshot c·ª• th·ªÉ
spark.sql(f"""
    CALL demo.system.rollback_to_snapshot(
        'demo.bronze.transactions',
        {old_snapshot_id}
    )
""")
```

### 5.7 C√°c use cases c·ªßa Time Travel

| Use Case            | M√¥ t·∫£                             | V√≠ d·ª•                      |
| ------------------- | --------------------------------- | -------------------------- |
| **Data Recovery**   | Kh√¥i ph·ª•c sau khi x√≥a/update nh·∫ßm | Rollback v·ªÅ snapshot tr∆∞·ªõc |
| **Audit**           | Xem d·ªØ li·ªáu t·∫°i th·ªùi ƒëi·ªÉm c·ª• th·ªÉ  | Query as-of timestamp      |
| **Debugging**       | T√¨m khi n√†o data b·ªã thay ƒë·ªïi      | So s√°nh snapshots          |
| **Reproducibility** | Ch·∫°y l·∫°i analysis v·ªõi data c≈©     | Query specific snapshot    |
| **Testing**         | Test v·ªõi production data snapshot | T·∫°o test t·ª´ snapshot       |

---

## 6. T·∫°o Dashboard Trong Superset

### 6.1 ƒêƒÉng nh·∫≠p Superset

1. M·ªü browser: **http://localhost:8088**
2. ƒêƒÉng nh·∫≠p: `admin` / `admin`

### 6.2 T·∫°o Database Connection (n·∫øu ch∆∞a c√≥)

1. V√†o **Settings** (‚öôÔ∏è) ‚Üí **Database Connections**
2. Click **+ Database**
3. Ch·ªçn **ClickHouse Connect**
4. Nh·∫≠p connection string:
   ```
   clickhousedb://default:clickhouse123@clickhouse:8123/fraud_detection
   ```
5. Click **Test Connection**
6. Click **Connect**

### 6.3 T·∫°o Dataset

1. V√†o **SQL Lab** ‚Üí **SQL Editor**
2. Ch·ªçn Database: `ClickHouse Fraud Detection`
3. Ch·ªçn Schema: `fraud_detection`
4. Th·∫•y c√°c tables:
   - `fraud_by_card_type`
   - `hourly_fraud_analysis`
   - `daily_transaction_summary`
   - ... v√† c√°c tables kh√°c

5. Click **+ Dataset** ƒë·ªÉ t·∫°o dataset t·ª´ table

### 6.4 T·∫°o Charts

**Chart 1: T·ªïng s·ªë giao d·ªãch (Big Number)**

- Dataset: `kpi_summary`
- Metric: `total_transactions`
- Chart type: Big Number

**Chart 2: T·ª∑ l·ªá gian l·∫≠n (Gauge)**

- Dataset: `kpi_summary`
- Metric: `fraud_rate_pct`
- Chart type: Gauge Chart

**Chart 3: Gian l·∫≠n theo lo·∫°i th·∫ª (Bar Chart)**

- Dataset: `fraud_by_card_type`
- Dimension: `card_brand`
- Metrics: `fraud_count`, `legitimate_count`
- Chart type: Bar Chart

**Chart 4: Xu h∆∞·ªõng theo ng√†y (Line Chart)**

- Dataset: `daily_transaction_summary`
- Dimension: `transaction_day`
- Metrics: `fraud_count`, `total_transactions`
- Chart type: Time-series Line Chart

### 6.5 T·∫°o Dashboard

1. V√†o **Dashboards** ‚Üí **+ Dashboard**
2. ƒê·∫∑t t√™n: "Fraud Detection Dashboard"
3. K√©o th·∫£ c√°c Charts v√†o Dashboard
4. S·∫Øp x·∫øp layout
5. **Save**

---

## 7. D·ª´ng H·ªá Th·ªëng

### 7.1 D·ª´ng t·∫°m th·ªùi (gi·ªØ data)

```bash
./scripts/stop_lakehouse.sh
```

Ho·∫∑c:

```bash
docker compose stop
```

### 7.2 D·ª´ng v√† x√≥a containers (gi·ªØ data trong volumes)

```bash
docker compose down
```

### 7.3 D·ª´ng v√† x√≥a to√†n b·ªô (bao g·ªìm data)

‚ö†Ô∏è **C·∫£nh b√°o**: L·ªánh n√†y s·∫Ω x√≥a t·∫•t c·∫£ d·ªØ li·ªáu!

```bash
docker compose down -v
```

### 7.4 Kh·ªüi ƒë·ªông l·∫°i

```bash
# N·∫øu mu·ªën gi·ªØ data c≈©
./scripts/start_lakehouse.sh

# N·∫øu mu·ªën ch·∫°y l·∫°i t·ª´ ƒë·∫ßu
docker compose down -v
./scripts/run_full_pipeline.sh
```

---

## 8. X·ª≠ L√Ω S·ª± C·ªë

### 8.1 Docker kh√¥ng ch·∫°y

**Tri·ªáu ch·ª©ng**: `Cannot connect to the Docker daemon`

**Gi·∫£i ph√°p**:

```bash
# Linux
sudo systemctl start docker

# Mac/Windows
# M·ªü Docker Desktop
```

### 8.2 Container kh√¥ng kh·ªüi ƒë·ªông

**Tri·ªáu ch·ª©ng**: Container exit v·ªõi error

**Gi·∫£i ph√°p**:

```bash
# Xem logs
docker logs spark-iceberg
docker logs clickhouse
docker logs superset

# Restart container c·ª• th·ªÉ
docker restart spark-iceberg
```

### 8.3 Thrift Server kh√¥ng kh·ªüi ƒë·ªông

**Tri·ªáu ch·ª©ng**: dbt connection failed, port 10000 kh√¥ng m·ªü

**Gi·∫£i ph√°p**:

```bash
# Xem logs Thrift Server
docker exec spark-iceberg cat /opt/spark/logs/*HiveThriftServer2*.out | tail -50

# Stop v√† restart
docker exec spark-iceberg /opt/spark/sbin/stop-thriftserver.sh
./scripts/start_lakehouse.sh
```

### 8.4 dbt run failed

**Tri·ªáu ch·ª©ng**: `dbt run` b√°o l·ªói

**Gi·∫£i ph√°p**:

```bash
# Test connection
docker exec dbt dbt debug

# Xem chi ti·∫øt l·ªói
docker exec dbt dbt run --debug

# Ch·∫°y l·∫°i model c·ª• th·ªÉ
docker exec dbt dbt run --select silver_transactions
```

### 8.5 ClickHouse connection failed

**Tri·ªáu ch·ª©ng**: Kh√¥ng query ƒë∆∞·ª£c ClickHouse

**Gi·∫£i ph√°p**:

```bash
# Ki·ªÉm tra ClickHouse
curl "http://localhost:8123/?query=SELECT%201"

# Restart ClickHouse
docker restart clickhouse

# Ch·ªù 10 gi√¢y v√† th·ª≠ l·∫°i
sleep 10
curl "http://localhost:8123/?query=SELECT%201"
```

### 8.6 Superset kh√¥ng load ƒë∆∞·ª£c charts

**Tri·ªáu ch·ª©ng**: Dashboard tr·ªëng, charts kh√¥ng hi·ªÉn th·ªã

**Gi·∫£i ph√°p**:

1. Ki·ªÉm tra Database connection trong Superset
2. V√†o **Settings ‚Üí Database Connections**
3. Click v√†o database ‚Üí **Test Connection**
4. N·∫øu fail, ki·ªÉm tra ClickHouse ƒëang ch·∫°y

### 8.7 Thi·∫øu data

**Tri·ªáu ch·ª©ng**: Tables tr·ªëng, kh√¥ng c√≥ records

**Gi·∫£i ph√°p**:

```bash
# Ki·ªÉm tra Bronze tables c√≥ data
docker exec spark-iceberg /opt/spark/bin/spark-sql \
    -e "SELECT COUNT(*) FROM demo.bronze.transactions"

# N·∫øu tr·ªëng, ch·∫°y l·∫°i Bronze layer
./scripts/run_full_pipeline.sh
```

### 8.8 Reset ho√†n to√†n

**Khi t·∫•t c·∫£ c√°ch kh√°c kh√¥ng ƒë∆∞·ª£c**:

```bash
# D·ª´ng v√† x√≥a t·∫•t c·∫£
docker compose down -v

# X√≥a cache Docker (t√πy ch·ªçn)
docker system prune -a

# Ch·∫°y l·∫°i t·ª´ ƒë·∫ßu
./scripts/run_full_pipeline.sh
```

---

## üìû H·ªó Tr·ª£

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ kh√¥ng gi·∫£i quy·∫øt ƒë∆∞·ª£c, h√£y:

1. Ki·ªÉm tra logs: `docker logs <container_name>`
2. Xem l·∫°i c√°c b∆∞·ªõc trong h∆∞·ªõng d·∫´n
3. Reset v√† ch·∫°y l·∫°i t·ª´ ƒë·∫ßu

---

## ‚úÖ Checklist Ho√†n Th√†nh

- [ ] Docker ƒëang ch·∫°y
- [ ] Data files c√≥ trong `notebooks/data/`
- [ ] Pipeline ho√†n th√†nh (C√°ch 1 ho·∫∑c C√°ch 2)
- [ ] Truy c·∫≠p Superset Dashboard th√†nh c√¥ng
- [ ] Th·ª≠ nghi·ªám Time Travel v·ªõi Notebook 5

**üéâ Ch√∫c m·ª´ng! B·∫°n ƒë√£ thi·∫øt l·∫≠p th√†nh c√¥ng Lakehouse Platform!**
