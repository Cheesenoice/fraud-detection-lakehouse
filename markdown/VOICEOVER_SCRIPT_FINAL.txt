Xin chào ban giám khảo và các bạn. Tôi là kỹ sư dữ liệu chịu trách nhiệm chính thiết kế và xây dựng hệ thống này. Trong kỷ nguyên Cách mạng Công nghiệp 4.0, dữ liệu không chỉ là con số, mà là huyết mạch của doanh nghiệp. Đặc biệt trong lĩnh vực Tài chính Ngân hàng Fintech, nơi dòng tiền luân chuyển liên tục từng giây, thì bài toán Phát hiện gian lận thẻ tín dụng Credit Card Fraud Detection trở thành một thách thức sống còn. Các giải pháp truyền thống như Data Warehouse thường gặp khó khăn về chi phí khi mở rộng dung lượng, hoặc không xử lý tốt dữ liệu phi cấu trúc. Các giải pháp Cloud Managed như AWS, Snowflake thì cực kỳ đắt đỏ và gây ra tình trạng Vendor Lock-in. Vì lẽ đó, tôi xin giới thiệu giải pháp Fullstack Open Source Data Lakehouse. Đây là một nền tảng dữ liệu hợp nhất, kết hợp ưu điểm lưu trữ rẻ tiền của Data Lake và khả năng quản lý chặt chẽ của Data Warehouse.

Hệ thống này được tôi tự xây dựng hoàn toàn từ con số 0, chạy hoàn toàn On premise trên nền tảng Docker. Về công nghệ lõi, tôi lựa chọn MinIO thay thế hoàn hảo cho AWS S3 để lưu trữ Object Storage. Apache Iceberg, công nghệ Table Format đang làm mưa làm gió hiện nay, giúp mang lại tính năng ACID Transaction cho hồ dữ liệu, điều mà trước đây chỉ database truyền thống mới có. Apache Spark, cỗ máy cơ bắp để xử lý tính toán phân tán. dbt, viết tắt của data build tool, để quản lý các luồng chuyển đổi dữ liệu Transformation một cách logic và khoa học. ClickHouse dùng cho tầng Serving nhờ tốc độ truy vấn OLAP siêu nhanh. Và cuối cùng là Apache Superset để trực quan hóa dữ liệu.

Một trong những tiêu chí quan trọng nhất của DevOps hiện đại là tính tự động hóa Automation và khả năng tái lập Reproducibility. Thay vì cấu hình thủ công từng bước rất dễ sai sót, tôi đã đóng gói toàn bộ quy trình DevOps vào một script duy nhất tên là run full pipeline chấm sh. Khi tôi chạy script này, các bạn sẽ thấy một chuỗi các hành động được kích hoạt tự động. Đầu tiên là Step 0, Dựng hạ tầng. Script sẽ gọi Docker Compose để khởi tạo 7 containers, thiết lập mạng nội bộ để các dịch vụ Spark, MinIO, ClickHouse giao tiếp mật thiết với nhau. Tiếp theo là Step 1, Ingestion. Hãy xem file code bronze layer chấm py. Tôi sử dụng Spark để đọc hàng triệu bản ghi từ file CSV thô. Thay vì lưu trữ lại dưới dạng file CSV hay Parquet thông thường, tôi chuyển đổi chúng ngay lập tức sang định dạng Apache Iceberg. Tại sao lại là Iceberg. Vì nó giúp chúng ta quản lý dữ liệu theo các Snapshots. Dữ liệu tại tầng Bronze này được giữ nguyên bản nhất có thể, tôi chỉ thêm vài trường metadata như ingestion time thời gian nạp và source file để phục vụ việc truy vết nguồn gốc sau này Data Lineage. Các bạn có thể thấy trên log, hàng trăm nghìn giao dịch đã được nạp thành công vào MinIO chỉ trong vài giây.

Sau khi nạp xong, chúng ta đến với Step 2, Transformation. Đây là trái tim của kiến trúc Medallion gồm Bronze Silver và Gold. Tại tầng Silver Layer, tôi sử dụng dbt kết hợp với Spark. Hãy nhìn vào code SQL của dbt. Đây là nơi diễn ra các logic làm sạch dữ liệu phức tạp. Ví dụ, tôi xử lý các giá trị Null bằng cách điền giá trị trung vị Median giúp tránh làm lệch phân phối dữ liệu. Tôi chuẩn hóa các định dạng thời gian từ giây sang format DateTime chuẩn. Đặc biệt, tôi thực hiện Feature Engineering, tạo thêm các cột mới như day of week ngày trong tuần hay is high risk cờ rủi ro cao ngay tại tầng này. Một điểm mạnh sát thủ của Iceberg phát huy tác dụng ở đây là Schema Evolution. Giả sử ngày mai Business yêu cầu thêm một cột Ví điện tử vào dữ liệu nguồn. Với kiến trúc cũ, ta phải đập đi xây lại bảng. Nhưng với Iceberg, bảng dữ liệu tự động thích ứng với cấu trúc mới mà không làm gián đoạn hệ thống.

Tiếp đến là Gold Layer. Đây là tầng dữ liệu tinh gọn phục vụ báo cáo. Trong file kpi summary chấm sql, tôi thực hiện các phép tổng hợp Aggregation như tính tổng số tiền gian lận, tỷ lệ gian lận theo ngày. Để tối ưu hiệu năng, tôi áp dụng kỹ thuật Partitioning phân vùng dữ liệu theo ngày và Z Ordering sắp xếp dữ liệu đa chiều trong Iceberg, giúp Spark bỏ qua các file không cần thiết khi truy vấn, tăng tốc độ xử lý lên gấp nhiều lần.

Dữ liệu đã sạch và đẹp. Nhưng làm sao để người dùng cuối xem được nhanh nhất. Chúng ta đến với Step 3, Serving Layer. Tôi không cho người dùng query trực tiếp vào Spark hay MinIO vì độ trễ cao. Thay vào đó, tôi đồng bộ các bảng Gold sang ClickHouse. Trong script serving layer chấm py, các bạn thấy tôi sử dụng cơ chế insert batch cực nhanh. ClickHouse là cơ sở dữ liệu dạng cột Columnar Database tối ưu cho các truy vấn phân tích OLAP. Nó cho phép phản hồi kết quả gần như tức thì, low latency. Và cuối cùng, Step 4, Auto Visualization. Thay vì phải vào Superset kéo thả từng biểu đồ thủ công, tôi viết script setup superset chấm py sử dụng API của Superset để tự động kết nối tới ClickHouse, tự động định nghĩa các Dataset, tự động vẽ 8 loại biểu đồ khác nhau, và sắp xếp chúng vào một Dashboard hoàn chỉnh.

Đây là kết quả cuối cùng. Một Dashboard chuyên nghiệp được sinh ra hoàn toàn tự động. Hãy cùng phân tích các chỉ số Business Insights. KPI Summary, chúng ta thấy ngay con số quan trọng nhất là Fraud Rate Tỷ lệ gian lận. Nếu con số này vượt ngưỡng an toàn, hệ thống cảnh báo ngay. Hourly Analysis, biểu đồ này cực kỳ giá trị cho đội vận hành. Nó chỉ ra rằng gian lận thường tăng đột biến vào khung giờ 2 đến 3 giờ sáng lúc mọi người đang ngủ. Fraud by Product and Card, giúp nhận diện rủi ro từ đâu, Thẻ Visa hay Master, Dòng sản phẩm nào đang bị lợi dụng rửa tiền.

Trước khi kết thúc, tôi muốn demo một tính năng mà tôi tâm đắc nhất là Time Travel. Trong vận hành thực tế, việc pipeline chạy sai làm hỏng dữ liệu là chuyện cơm bữa. Với Data Warehouse cũ, việc khôi phục restore mất hàng giờ đồng hồ. Nhưng với Iceberg, tôi có thể du hành thời gian. Tôi có thể truy vấn lại chính xác dữ liệu của ngày hôm qua, hoặc 1 tiếng trước bằng tính năng Snapshots. Và nếu cần, tôi chỉ cần chạy lệnh CALL demo system rollback to snapshot. Bùm. Toàn bộ bảng dữ liệu quay ngược về trạng thái đúng đắn trong tích tắc. Đây là tính năng bảo hiểm tuyệt vời cho dữ liệu tài chính.

Tổng kết lại, giải pháp Data Lakehouse này đã giải quyết trọn vẹn bài toán đặt ra. Tự chủ công nghệ 100% Open source, không tốn phí bản quyền. Hiệu năng cao kết hợp sức mạnh của Spark xử lý lớn và ClickHouse truy vấn nhanh. Tin cậy đảm bảo tính ACID và khả năng Time Travel nhờ Iceberg. Tự động hóa triển khai một chạm One click deployment. Đây là nền tảng vững chắc để doanh nghiệp mở rộng quy mô dữ liệu trong tương lai. Cảm ơn ban giám khảo và các bạn đã lắng nghe.
