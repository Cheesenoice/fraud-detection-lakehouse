#!/bin/bash

###############################################################################
# FULL LAKEHOUSE PIPELINE AUTOMATION - ONE COMMAND
# Credit Card Fraud Detection - IEEE-CIS Dataset
# Chá»‰ cáº§n 1 lá»‡nh: ./run_full_pipeline.sh
###############################################################################

set -e

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m'

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"

echo -e "${CYAN}"
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘         ğŸš€ FULL LAKEHOUSE PIPELINE - ONE COMMAND                     â•‘"
echo "â•‘         Credit Card Fraud Detection - IEEE-CIS Dataset               â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo -e "${NC}"

###############################################################################
# STEP 0: Docker Compose Up
###############################################################################
echo -e "\n${YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
echo -e "${BLUE}ğŸ³ STEP 0: Khá»Ÿi Ä‘á»™ng Docker Stack${NC}"
echo -e "${YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"

# Check Docker
if ! docker info > /dev/null 2>&1; then
    echo -e "${RED}âŒ Docker khÃ´ng cháº¡y. Vui lÃ²ng khá»Ÿi Ä‘á»™ng Docker trÆ°á»›c.${NC}"
    exit 1
fi
echo -e "${GREEN}âœ… Docker daemon Ä‘ang cháº¡y${NC}"

# Start containers
cd "$PROJECT_DIR"
echo "   ğŸ”„ Äang khá»Ÿi Ä‘á»™ng containers..."
docker compose up -d

# Wait for containers to be ready
echo "   â³ Äá»£i containers khá»Ÿi Ä‘á»™ng..."
REQUIRED_CONTAINERS=("spark-iceberg" "clickhouse" "dbt" "superset" "minio" "iceberg-rest")
MAX_WAIT=120
WAIT_TIME=0

while [ $WAIT_TIME -lt $MAX_WAIT ]; do
    ALL_RUNNING=true
    for container in "${REQUIRED_CONTAINERS[@]}"; do
        if ! docker ps --format '{{.Names}}' | grep -q "^${container}$"; then
            ALL_RUNNING=false
            break
        fi
    done
    
    if $ALL_RUNNING; then
        break
    fi
    
    sleep 5
    WAIT_TIME=$((WAIT_TIME + 5))
    echo -n "."
done

if [ $WAIT_TIME -ge $MAX_WAIT ]; then
    echo -e "\n${RED}âŒ Timeout: Containers khÃ´ng khá»Ÿi Ä‘á»™ng ká»‹p${NC}"
    exit 1
fi

echo -e "\n${GREEN}âœ… Táº¥t cáº£ containers Ä‘ang cháº¡y${NC}"

# Start Thrift Server for dbt
echo "   ğŸ”„ Khá»Ÿi Ä‘á»™ng Spark Thrift Server..."

# Stop any existing Thrift Server first
docker exec spark-iceberg /opt/spark/sbin/stop-thriftserver.sh 2>/dev/null || true
sleep 3

# Start Thrift Server with proper config (full Iceberg + S3 settings)
docker exec -d spark-iceberg /opt/spark/sbin/start-thriftserver.sh \
    --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
    --conf spark.sql.catalog.demo=org.apache.iceberg.spark.SparkCatalog \
    --conf spark.sql.catalog.demo.type=rest \
    --conf spark.sql.catalog.demo.uri=http://iceberg-rest:8181 \
    --conf spark.sql.catalog.demo.io-impl=org.apache.iceberg.aws.s3.S3FileIO \
    --conf spark.sql.catalog.demo.warehouse=s3://warehouse/ \
    --conf spark.sql.catalog.demo.s3.endpoint=http://minio:9000 \
    --conf spark.sql.catalog.demo.s3.path-style-access=true \
    --conf spark.sql.catalog.demo.s3.access-key-id=admin \
    --conf spark.sql.catalog.demo.s3.secret-access-key=password123 \
    --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 \
    --conf spark.hadoop.fs.s3a.access.key=admin \
    --conf spark.hadoop.fs.s3a.secret.key=password123 \
    --conf spark.hadoop.fs.s3a.path.style.access=true \
    --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
    --conf spark.sql.defaultCatalog=demo \
    --hiveconf hive.server2.thrift.port=10000

# Wait for Thrift Server to be ready (check port 10000)
echo "   â³ Äá»£i Thrift Server sáºµn sÃ ng (tá»‘i Ä‘a 90 giÃ¢y)..."
THRIFT_WAIT=0
THRIFT_MAX=90
while [ $THRIFT_WAIT -lt $THRIFT_MAX ]; do
    # Check if port 10000 is open using Python
    if docker exec dbt python3 -c "import socket; s=socket.socket(); s.settimeout(2); exit(0 if s.connect_ex(('spark-iceberg', 10000))==0 else 1)" 2>/dev/null; then
        echo -e "\n${GREEN}âœ… Thrift Server Ä‘Ã£ sáºµn sÃ ng trÃªn port 10000${NC}"
        break
    fi
    sleep 5
    THRIFT_WAIT=$((THRIFT_WAIT + 5))
    echo -n "."
done

if [ $THRIFT_WAIT -ge $THRIFT_MAX ]; then
    echo -e "\n${RED}âŒ Timeout: Thrift Server khÃ´ng khá»Ÿi Ä‘á»™ng Ä‘Æ°á»£c${NC}"
    echo "   Xem log: docker exec spark-iceberg tail -50 /opt/spark/logs/*HiveThriftServer2*.out"
    exit 1
fi

# Check data files
if [ ! -f "$PROJECT_DIR/notebooks/data/train_transaction.csv" ]; then
    echo -e "${RED}âŒ KhÃ´ng tÃ¬m tháº¥y train_transaction.csv${NC}"
    exit 1
fi
echo -e "${GREEN}âœ… Data files sáºµn sÃ ng${NC}"

###############################################################################
# STEP 1: Bronze Layer
###############################################################################
echo -e "\n${YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
echo -e "${BLUE}ğŸ“¥ STEP 1: Bronze Layer - Ingest CSV vÃ o Iceberg${NC}"
echo -e "${YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"

# Create namespaces if they don't exist (required for first run)
echo "   ğŸ”„ Táº¡o namespaces trong Iceberg catalog..."
docker cp "$SCRIPT_DIR/create_namespaces.py" spark-iceberg:/tmp/create_namespaces.py
docker exec spark-iceberg /opt/spark/bin/spark-submit --master local[*] \
    --conf spark.sql.catalog.demo=org.apache.iceberg.spark.SparkCatalog \
    --conf spark.sql.catalog.demo.type=rest \
    --conf spark.sql.catalog.demo.uri=http://iceberg-rest:8181 \
    --conf spark.sql.defaultCatalog=demo \
    /tmp/create_namespaces.py 2>&1 | grep -E "(âœ…|âš ï¸|ğŸ“‹|Done)" || true
echo -e "   ${GREEN}âœ… Namespaces Ä‘Ã£ sáºµn sÃ ng${NC}"

# Copy Bronze script to container
docker cp "$SCRIPT_DIR/bronze_layer.py" spark-iceberg:/tmp/bronze_layer.py

echo "   ğŸ”„ Äang cháº¡y Bronze Layer ingestion..."
docker exec spark-iceberg /opt/spark/bin/spark-submit --master local[*] /tmp/bronze_layer.py

if [ $? -eq 0 ]; then
    echo -e "${GREEN}âœ… Bronze Layer hoÃ n táº¥t!${NC}"
else
    echo -e "${RED}âŒ Bronze Layer tháº¥t báº¡i!${NC}"
    exit 1
fi

###############################################################################
# STEP 2: dbt run
###############################################################################
echo -e "\n${YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
echo -e "${BLUE}ğŸ”§ STEP 2: dbt run - Táº¡o Silver & Gold Layers${NC}"
echo -e "${YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"

echo "   ğŸ”„ Äang cháº¡y dbt run..."
docker exec dbt dbt run --project-dir /usr/app

if [ $? -eq 0 ]; then
    echo -e "${GREEN}âœ… dbt run hoÃ n táº¥t! (Silver + Gold layers)${NC}"
else
    echo -e "${RED}âŒ dbt run tháº¥t báº¡i!${NC}"
    exit 1
fi

###############################################################################
# STEP 3: Serving Layer
###############################################################################
echo -e "\n${YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
echo -e "${BLUE}ğŸ“¤ STEP 3: Serving Layer - Copy Gold sang ClickHouse${NC}"
echo -e "${YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"

# Copy Serving script to container
docker cp "$SCRIPT_DIR/serving_layer.py" spark-iceberg:/tmp/serving_layer.py

echo "   ğŸ”„ Äang cháº¡y Serving Layer..."
docker exec spark-iceberg /opt/spark/bin/spark-submit --master local[*] /tmp/serving_layer.py

if [ $? -eq 0 ]; then
    echo -e "${GREEN}âœ… Serving Layer hoÃ n táº¥t!${NC}"
else
    echo -e "${RED}âŒ Serving Layer tháº¥t báº¡i!${NC}"
    exit 1
fi

###############################################################################
# STEP 4: Superset Auto Setup
###############################################################################
echo -e "\n${YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
echo -e "${BLUE}ğŸ“Š STEP 4: Superset - Tá»± Ä‘á»™ng táº¡o Dashboard${NC}"
echo -e "${YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"

echo "   ğŸ”„ Äá»£i Superset khá»Ÿi Ä‘á»™ng hoÃ n táº¥t..."
sleep 10

# Check and install clickhouse-connect driver if needed
echo "   ğŸ” Kiá»ƒm tra ClickHouse driver trong Superset..."
if docker exec superset python3 -c "import clickhouse_connect" 2>/dev/null; then
    echo -e "   ${GREEN}âœ… clickhouse-connect driver Ä‘Ã£ cÃ³ sáºµn${NC}"
else
    echo "   ğŸ“¦ Äang cÃ i Ä‘áº·t clickhouse-connect driver..."
    docker exec -u root superset bash -c "apt-get update -qq && apt-get install -y -qq curl > /dev/null 2>&1 && curl -sS https://bootstrap.pypa.io/get-pip.py | python > /dev/null 2>&1 && pip install clickhouse-connect > /dev/null 2>&1" 2>/dev/null
    
    if docker exec superset python3 -c "import clickhouse_connect" 2>/dev/null; then
        echo -e "   ${GREEN}âœ… clickhouse-connect driver Ä‘Ã£ cÃ i Ä‘áº·t thÃ nh cÃ´ng${NC}"
        echo "   ğŸ”„ Äang restart Superset Ä‘á»ƒ load driver..."
        docker restart superset > /dev/null 2>&1
        
        # Wait for Superset to be ready after restart
        echo "   â³ Äá»£i Superset khá»Ÿi Ä‘á»™ng láº¡i (60 giÃ¢y)..."
        SUPERSET_WAIT=0
        SUPERSET_MAX=90
        while [ $SUPERSET_WAIT -lt $SUPERSET_MAX ]; do
            if curl -s "http://localhost:8088/health" 2>/dev/null | grep -q "OK"; then
                echo -e "\n   ${GREEN}âœ… Superset Ä‘Ã£ sáºµn sÃ ng${NC}"
                break
            fi
            sleep 5
            SUPERSET_WAIT=$((SUPERSET_WAIT + 5))
            echo -n "."
        done
        
        if [ $SUPERSET_WAIT -ge $SUPERSET_MAX ]; then
            echo -e "\n   ${YELLOW}âš ï¸  Superset chÆ°a sáºµn sÃ ng, tiáº¿p tá»¥c...${NC}"
        fi
    else
        echo -e "   ${YELLOW}âš ï¸  KhÃ´ng thá»ƒ cÃ i clickhouse-connect driver${NC}"
    fi
fi

# Run Superset setup script
echo "   ğŸ”„ Äang setup Superset (Database, Datasets, Charts, Dashboard)..."
python3 "$SCRIPT_DIR/setup_superset.py"

if [ $? -eq 0 ]; then
    echo -e "${GREEN}âœ… Superset setup hoÃ n táº¥t!${NC}"
else
    echo -e "${YELLOW}âš ï¸  Superset auto-setup cÃ³ lá»—i. Báº¡n cÃ³ thá»ƒ setup thá»§ cÃ´ng:${NC}"
    echo -e "${CYAN}
ğŸ“Œ Äá»ƒ táº¡o Dashboard trong Superset thá»§ cÃ´ng:
1. Má»Ÿ browser: http://localhost:8088
2. ÄÄƒng nháº­p: admin / admin
3. Settings â†’ Database Connections â†’ + Database
4. Chá»n 'ClickHouse Connect'
5. Connection string:
   clickhousedb://default:clickhouse123@clickhouse:8123/fraud_detection
6. Test Connection â†’ Connect
7. VÃ o SQL Lab Ä‘á»ƒ query hoáº·c táº¡o Charts

ğŸ“– Xem hÆ°á»›ng dáº«n chi tiáº¿t: ${PROJECT_DIR}/markdown/SUPERSET_CHARTS_GUIDE.md
${NC}"
fi

###############################################################################
# FINAL SUMMARY
###############################################################################
echo -e "\n${GREEN}"
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘                    ğŸ‰ PIPELINE HOÃ€N Táº¤T!                             â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo -e "${NC}"

echo -e "${CYAN}ğŸ“Š Káº¾T QUáº¢:${NC}"
echo "   âœ… Bronze Layer: Raw CSV â†’ Iceberg tables (demo.bronze.*)"
echo "   âœ… Silver Layer: Cleaned data (demo.silver.*)"
echo "   âœ… Gold Layer: Analytics tables (demo.gold.*)"
echo "   âœ… Serving Layer: ClickHouse tables (fraud_detection.*)"

echo -e "\n${CYAN}ğŸŒ TRUY Cáº¬P:${NC}"
echo "   ğŸ“Š Superset Dashboard: http://localhost:8088 (admin/admin)"
echo "   ğŸ“ MinIO Console:     http://localhost:9001 (admin/password123)"
echo "   ğŸ’» Jupyter:           http://localhost:8888"
echo "   ğŸ—„ï¸  ClickHouse:        http://localhost:8123"

echo -e "\n${YELLOW}â±ï¸  HoÃ n táº¥t: $(date)${NC}"
echo ""
